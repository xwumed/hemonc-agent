"""
Agentè¯„ä¼°è„šæœ¬

åŠŸèƒ½ï¼š
- åˆå¹¶agent_results.csvå’ŒListFinalnew.xlsxï¼ˆground truthï¼‰
- ä½¿ç”¨GPT-4oå¯¹agentçš„final_decisionè¿›è¡Œç»“æ„åŒ–è¯„åˆ†
- è¯„åˆ†ç»´åº¦ï¼šæ²»ç–—æ–¹æ¡ˆä¸€è‡´æ€§ã€ä¸´åºŠæ¨ç†è´¨é‡ã€å®‰å…¨æ€§è€ƒé‡ã€æŒ‡å—å‚è€ƒå‡†ç¡®æ€§ã€å®Œæ•´æ€§
- ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨

ä½¿ç”¨æ–¹æ³•ï¼š
python evaluate.py [--agent-csv PATH] [--list-final PATH] [--output-dir DIR]
"""

import os
import json
import asyncio
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import warnings
from typing import Optional, Dict, Any

# é…ç½®ç®¡ç†å™¨
from config_manager import ConfigManager

# OpenAIå®¢æˆ·ç«¯
from openai import AsyncOpenAI

# å¯è§†åŒ–åº“
try:
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica']
    plt.rcParams['axes.unicode_minus'] = False
    PLOT_AVAILABLE = True
except ImportError:
    PLOT_AVAILABLE = False
    warnings.warn("matplotlibæœªå®‰è£…ï¼Œå°†è·³è¿‡å›¾è¡¨ç”Ÿæˆã€‚å®‰è£…: pip install matplotlib")

try:
    import seaborn as sns
    sns.set_style("whitegrid")
    SEABORN_AVAILABLE = True
except ImportError:
    SEABORN_AVAILABLE = False
    warnings.warn("seabornæœªå®‰è£…ï¼Œå°†ä½¿ç”¨matplotlibç»˜å›¾ã€‚å®‰è£…: pip install seaborn")

# ç»Ÿè®¡æ£€éªŒåº“
try:
    from scipy import stats
    STATS_AVAILABLE = True
except ImportError:
    STATS_AVAILABLE = False
    warnings.warn("scipyæœªå®‰è£…ï¼Œå°†è·³è¿‡ç»Ÿè®¡æ£€éªŒã€‚å®‰è£…: pip install scipy")


# ============================================================================
# é…ç½®
# ============================================================================

# åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
config_manager = ConfigManager()

# ä»config.tomlåŠ è½½openai_gpt4oé…ç½®
gpt4o_config = config_manager.get_config("openai_gpt4o")

# OpenAIé…ç½®
API_BASE = gpt4o_config.get("api_base")
API_KEY = gpt4o_config.get("api_key")
OPENAI_MODEL = gpt4o_config.get("model_name", "gpt-4o")
MAX_TOKENS = gpt4o_config.get("max_tokens", 1500)
TIMEOUT = gpt4o_config.get("timeout", 300)
TEST_LIMIT = int(os.getenv("TEST_LIMIT", "0"))

# éªŒè¯APIé…ç½®
if not API_BASE or not API_KEY or API_KEY == "EMPTY":
    print("âš ï¸  è­¦å‘Š: OpenAI APIé…ç½®æœªæ‰¾åˆ°ï¼")
    print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("  - OPENAI_API_BASE: OpenAI APIåŸºç¡€URL")
    print("  - OPENAI_API_KEY: OpenAI APIå¯†é’¥")
    print("\næˆ–åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡ã€‚")
    sys.exit(1)

# è¯„ä¼°ç»´åº¦ï¼ˆ10åˆ†åˆ¶ï¼‰
DIMENSIONS = [
    "treatment_match",
    "clinical_reasoning",
    "safety_awareness",
    "guideline_compliance",
    "completeness"
]

DIMENSION_NAMES = {
    "treatment_match": "Treatment Match",
    "clinical_reasoning": "Clinical Reasoning",
    "safety_awareness": "Safety Awareness",
    "guideline_compliance": "Guideline Compliance",
    "completeness": "Completeness",
    "overall": "Overall (Weighted Avg)"
}

# ç»´åº¦æƒé‡ï¼ˆç”¨äºè®¡ç®—overallåŠ æƒå¹³å‡ï¼‰
DIMENSION_WEIGHTS = {
    "treatment_match": 0.40,        # æ ¸å¿ƒï¼šä¸ground truthå¯¹æ¯”
    "clinical_reasoning": 0.20,     # ç‹¬ç«‹è¯„ä¼°ï¼šæ¨ç†è´¨é‡
    "safety_awareness": 0.15,       # ç‹¬ç«‹è¯„ä¼°ï¼šå®‰å…¨æ€§è€ƒé‡
    "guideline_compliance": 0.10,   # ç‹¬ç«‹è¯„ä¼°ï¼šæŒ‡å—å¼•ç”¨
    "completeness": 0.15            # ç‹¬ç«‹è¯„ä¼°ï¼šå®Œæ•´æ€§
}

# 10åˆ†åˆ¶èŒƒå›´
SCORE_MIN = 1
SCORE_MAX = 10

# ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„å¼‚æ­¥å®¢æˆ·ç«¯
client = config_manager.async_external_client

# å¦‚æœéœ€è¦ä½¿ç”¨openai_gpt4oé…ç½®ï¼Œåˆ›å»ºä¸“ç”¨å®¢æˆ·ç«¯
# æ³¨æ„ï¼šconfig_manageré»˜è®¤ä½¿ç”¨local_gpté…ç½®ï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦openai_gpt4o
try:
    # åˆ›å»ºä½¿ç”¨openai_gpt4oé…ç½®çš„ä¸“ç”¨å®¢æˆ·ç«¯
    gpt4o_client = AsyncOpenAI(
        api_key=API_KEY,
        base_url=API_BASE,
        timeout=TIMEOUT
    )
    client = gpt4o_client
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}")
    print("è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡OPENAI_API_BASEå’ŒOPENAI_API_KEYæ˜¯å¦æ­£ç¡®è®¾ç½®ã€‚")
    sys.exit(1)


# ============================================================================
# 1. åˆå¹¶åŠŸèƒ½
# ============================================================================
def merge_agent_and_ground_truth(agent_csv: str, list_final_xlsx: str) -> pd.DataFrame:
    """
    åˆå¹¶agent_results.csvå’ŒListFinalnew.xlsx
    
    å‚æ•°:
        agent_csv: agentç»“æœCSVè·¯å¾„
        list_final_xlsx: ground truth Excelè·¯å¾„
    
    è¿”å›:
        åˆå¹¶åçš„DataFrame
    """
    print("=" * 80)
    print("ğŸ“Š å¼€å§‹åˆå¹¶æ•°æ®...")
    print("=" * 80)
    
    # è¯»å–agent_results.csv
    print(f"\n[1/2] è¯»å– {agent_csv}...")
    agent_path = Path(agent_csv)
    if not agent_path.exists():
        print(f"   âœ— æ–‡ä»¶ä¸å­˜åœ¨: {agent_csv}")
        sys.exit(1)
    
    df_agent = pd.read_csv(agent_path, encoding='utf-8-sig')
    print(f"   âœ“ Agentç»“æœ: {len(df_agent)} æ¡è®°å½•")
    print(f"   åˆ—å: {list(df_agent.columns)}")
    
    # è¯»å–ListFinalnew.xlsx
    print(f"\n[2/2] è¯»å– {list_final_xlsx}...")
    list_path = Path(list_final_xlsx)
    if not list_path.exists():
        print(f"   âœ— æ–‡ä»¶ä¸å­˜åœ¨: {list_final_xlsx}")
        sys.exit(1)
    
    df_list = pd.read_excel(list_path)
    print(f"   âœ“ Ground Truth: {len(df_list)} æ¡è®°å½•")
    print(f"   åˆ—å: {list(df_list.columns)}")
    
    # è¿‡æ»¤æ‰ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚final_decision_oldï¼‰
    # åªä¿ç•™patient_id, timestampå’Œground_truthç›¸å…³åˆ—
    keep_columns = ['patient_id', 'timestamp']
    # ä¿ç•™æ‰€æœ‰ground_truthå¼€å¤´çš„åˆ—
    keep_columns.extend([col for col in df_list.columns if col.startswith('ground_truth')])
    df_list = df_list[keep_columns]
    print(f"   ä¿ç•™åˆ—: {list(df_list.columns)}")
    
    # ç»Ÿä¸€æ•°æ®ç±»å‹
    df_agent['patient_id'] = df_agent['patient_id'].astype(str)
    df_agent['timestamp'] = df_agent['timestamp'].astype(str)
    df_list['patient_id'] = df_list['patient_id'].astype(str)
    df_list['timestamp'] = df_list['timestamp'].astype(str)
    
    # åˆå¹¶
    print("\n" + "=" * 80)
    print("åˆå¹¶æ•°æ®...")
    print("=" * 80)
    
    merged_df = pd.merge(
        df_agent,
        df_list,
        on=['patient_id', 'timestamp'],
        how='outer',
        suffixes=('_agent', '_list')
    )
    
    print(f"   âœ“ åˆå¹¶åè®°å½•æ•°: {len(merged_df)}")
    
    # æ’åº
    merged_df = merged_df.sort_values(['patient_id', 'timestamp'], ascending=[True, True])
    merged_df = merged_df.reset_index(drop=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 80)
    print("åˆå¹¶å®Œæˆï¼")
    print("=" * 80)
    print(f"æ€»è®°å½•æ•°: {len(merged_df)}")
    print(f"å”¯ä¸€æ‚£è€…æ•°: {merged_df['patient_id'].nunique()}")
    print(f"åˆ—æ•°: {len(merged_df.columns)}")
    
    return merged_df


# ============================================================================
# 2. è¯„ä¼°åŠŸèƒ½
# ============================================================================
async def evaluate_decision(
    decision: str,
    ground_truth: str,
    max_tokens: int = MAX_TOKENS
) -> Optional[Dict[str, Any]]:
    """ä½¿ç”¨GPT-4oè¯„ä¼°å†³ç­–ï¼ˆ10åˆ†åˆ¶ï¼‰"""
    prompt = f"""You are an expert hematological oncologist evaluating clinical decision quality.

GROUND TRUTH (Brief clinical decision summary):
{ground_truth}

DECISION TO EVALUATE:
{decision}

Score each dimension on a 1-10 scale (1=worst, 10=best):

{{
  "treatment_match": {{"score": <1-10>, "reason": "<brief reason>"}},
  "clinical_reasoning": {{"score": <1-10>, "reason": "<brief reason>"}},
  "safety_awareness": {{"score": <1-10>, "reason": "<brief reason>"}},
  "guideline_compliance": {{"score": <1-10>, "reason": "<brief reason>"}},
  "completeness": {{"score": <1-10>, "reason": "<brief reason>"}}
}}

DIMENSION DEFINITIONS:

=== A. COMPARATIVE EVALUATION (vs Ground Truth) ===

1. treatment_match (æ²»ç–—æ–¹æ¡ˆåŒ¹é…åº¦) [Weight: 40%]:
   Does the DECISION recommend the SAME core treatment as GROUND TRUTH?
   - Same drugs/regimen = high score
   - Similar approach but different drugs = medium score  
   - Different treatment strategy = low score

=== B. INDEPENDENT EVALUATION (Decision Quality) ===

2. clinical_reasoning (ä¸´åºŠæ¨ç†è´¨é‡) [Weight: 20%]:
   Is the clinical reasoning in the DECISION sound and evidence-based?
   Consider: disease staging, risk stratification, patient factors, logical coherence.

3. safety_awareness (å®‰å…¨æ€§è€ƒé‡) [Weight: 15%]:
   Does the DECISION adequately address safety considerations?
   Consider: contraindications, dose adjustments for organ dysfunction, drug interactions, monitoring needs.

4. guideline_compliance (æŒ‡å—å¼•ç”¨è´¨é‡) [Weight: 10%]:
   How specific and accurate are the guideline references in the DECISION?
   - Specific citations (version, section, evidence level) = high score
   - General references with some detail = medium score
   - Only generic mentions like "per NCCN/ESMO" = low score (â‰¤4)
   - No references = very low score

5. completeness (å®Œæ•´æ€§) [Weight: 15%]:
   How comprehensive is the DECISION?
   Consider coverage of: diagnosis, treatment plan, supportive care, follow-up, patient-specific considerations.

Be objective. Use the full 1-10 scale. Output ONLY valid JSON."""
    
    try:
        response = await client.responses.create(
            model=OPENAI_MODEL,
            input=prompt,
            instructions="You are an expert hematological oncologist. Always respond with valid JSON only."
        )
        
        content = response.output_text.strip()
        
        # å»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()
        
        result = json.loads(content)
        return result
    
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return None


def calculate_overall_score(eval_result: Dict[str, Any]) -> float:
    """è®¡ç®—äº”ä¸ªç»´åº¦çš„åŠ æƒå¹³å‡åˆ†ä½œä¸ºoverall score"""
    if not eval_result:
        return None
    
    total_weight = 0
    weighted_sum = 0
    
    for dim, weight in DIMENSION_WEIGHTS.items():
        if dim in eval_result and eval_result[dim].get("score") is not None:
            score = eval_result[dim]["score"]
            weighted_sum += score * weight
            total_weight += weight
    
    if total_weight > 0:
        return round(weighted_sum / total_weight, 2)
    return None


def flatten_evaluation(eval_result: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """å°†è¯„ä¼°ç»“æœæ‰å¹³åŒ–ä¸ºCSVåˆ—æ ¼å¼ï¼Œå¹¶è®¡ç®—overallåŠ æƒå¹³å‡"""
    if not eval_result:
        result = {}
        for dim in DIMENSIONS:
            result[f"{dim}_score"] = None
            result[f"{dim}_reason"] = "Evaluation failed"
        result["overall_score"] = None
        return result
    
    result = {}
    for dimension, values in eval_result.items():
        result[f"{dimension}_score"] = values.get("score")
        result[f"{dimension}_reason"] = values.get("reason", "")
    
    # è®¡ç®—åŠ æƒå¹³å‡ä½œä¸ºoverall score
    overall = calculate_overall_score(eval_result)
    result["overall_score"] = overall
    
    return result


async def process_row(row_id: int, row_data: pd.Series, total_rows: int, ground_truth_col: str) -> Optional[Dict[str, Any]]:
    """å¤„ç†å•è¡Œæ•°æ®ï¼Œè¯„ä¼°agentçš„å†³ç­–"""
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å¤„ç†è¡Œ {row_id}/{total_rows}")
    
    ground_truth = row_data.get(ground_truth_col, "")
    decision = row_data.get("final_decision", "")
    
    # æ£€æŸ¥æœ‰æ•ˆæ€§
    if pd.isna(ground_truth) or not str(ground_truth).strip():
        print(f"âš ï¸  è¡Œ {row_id}: Ground truthä¸ºç©ºï¼Œè·³è¿‡")
        return None
    
    if pd.isna(decision) or not str(decision).strip():
        print(f"âš ï¸  è¡Œ {row_id}: Agentå†³ç­–ä¸ºç©ºï¼Œè·³è¿‡")
        return None
    
    result = {
        "Row_ID": row_id,
        "patient_id": row_data.get("patient_id", ""),
        "timestamp": row_data.get("timestamp", ""),
        "final_decision": decision,
        "ground_truth": ground_truth
    }
    
    # è¯„ä¼°å†³ç­–
    print(f"ğŸ”„ è¯„ä¼°Agentå†³ç­–...")
    eval_result = await evaluate_decision(str(decision), str(ground_truth))
    scores = flatten_evaluation(eval_result)
    result.update(scores)
    
    if eval_result:
        overall = calculate_overall_score(eval_result)
        print(f"âœ… Agent Overall: {overall}/10")
    
    return result


async def run_evaluation(merged_df: pd.DataFrame, ground_truth_col: str) -> pd.DataFrame:
    """è¿è¡Œè¯„ä¼°æµç¨‹"""
    total_rows = len(merged_df)
    print(f"\nğŸ“Š å¼€å§‹è¯„ä¼°ï¼Œå…± {total_rows} è¡Œæ•°æ®")
    
    # åº”ç”¨æµ‹è¯•é™åˆ¶
    if TEST_LIMIT > 0:
        merged_df = merged_df.head(TEST_LIMIT)
        print(f"âš ï¸  æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {TEST_LIMIT} è¡Œ")
    
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {OPENAI_MODEL}")
    print("="*80)
    
    # å¤„ç†æ¯ä¸€è¡Œ
    results = []
    for idx, row in merged_df.iterrows():
        row_id = idx + 1
        result = await process_row(row_id, row, len(merged_df), ground_truth_col)
        if result:
            results.append(result)
        await asyncio.sleep(0.5)  # é¿å…APIé™æµ
    
    if not results:
        print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ•°æ®")
        return pd.DataFrame()
    
    result_df = pd.DataFrame(results)
    
    print("\n" + "="*80)
    print(f"âœ… è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(results)} è¡Œ")
    
    return result_df


# ============================================================================
# 3. åˆ†æåŠŸèƒ½
# ============================================================================
def analyze_scores(df: pd.DataFrame):
    """åˆ†æè¯„åˆ†æ•°æ®"""
    print("\n" + "="*80)
    print("ğŸ“Š è¯„åˆ†ç»Ÿè®¡åˆ†æ")
    print("="*80)
    
    print(f"\nã€Agent è¯„åˆ†ç»Ÿè®¡ã€‘")
    print("-" * 80)
    
    # äº”ä¸ªç»´åº¦
    for dim in DIMENSIONS:
        col = f"{dim}_score"
        if col in df.columns:
            scores = df[col].dropna()
            if len(scores) > 0:
                mean_score = scores.mean()
                weight_str = f"(æƒé‡{DIMENSION_WEIGHTS[dim]:.0%})" if dim in DIMENSION_WEIGHTS else ""
                print(f"  {DIMENSION_NAMES[dim]:20s}: {mean_score:5.2f}/10  "
                      f"(æœ€å°: {scores.min():.1f}, æœ€å¤§: {scores.max():.1f}, n={len(scores)}) {weight_str}")
    
    # OverallåŠ æƒå¹³å‡
    overall_col = "overall_score"
    if overall_col in df.columns:
        overall_scores = df[overall_col].dropna()
        if len(overall_scores) > 0:
            mean_overall = overall_scores.mean()
            print(f"  {'â”€'*60}")
            print(f"  {DIMENSION_NAMES['overall']:20s}: {mean_overall:5.2f}/10  "
                  f"(æœ€å°: {overall_scores.min():.1f}, æœ€å¤§: {overall_scores.max():.1f})")


def score_distribution(df: pd.DataFrame):
    """è¯„åˆ†åˆ†å¸ƒç»Ÿè®¡ï¼ˆ10åˆ†åˆ¶ï¼‰"""
    print("\n" + "="*80)
    print("ğŸ“Š Overallè¯„åˆ†åˆ†å¸ƒ")
    print("="*80)
    
    # 10åˆ†åˆ¶åˆ†å¸ƒåŒºé—´
    score_ranges = [(1, 3), (4, 6), (7, 8), (9, 10)]
    range_labels = ["ä½(1-3)", "ä¸­(4-6)", "è‰¯å¥½(7-8)", "ä¼˜ç§€(9-10)"]
    
    col = "overall_score"
    if col not in df.columns:
        return
    
    scores = df[col].dropna()
    if len(scores) == 0:
        return
    
    print(f"\nã€Agent Overallè¯„åˆ†åˆ†å¸ƒã€‘")
    print("-" * 40)
    for (low, high), label in zip(score_ranges, range_labels):
        count = ((scores >= low) & (scores <= high)).sum()
        percentage = (count / len(scores)) * 100
        bar = "â–ˆ" * int(percentage / 2)
        print(f"  {label:12s}: {count:3d} ({percentage:5.1f}%) {bar}")


# ============================================================================
# 4. å¯è§†åŒ–åŠŸèƒ½
# ============================================================================
def create_boxplots(df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆç®±çº¿å›¾"""
    if not PLOT_AVAILABLE:
        print("\nâš ï¸  matplotlibæœªå®‰è£…ï¼Œè·³è¿‡å›¾è¡¨ç”Ÿæˆ")
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š ç”Ÿæˆç®±çº¿å›¾")
    print("="*80)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å®šä¹‰é¢œè‰²
    color = '#3498db'  # è“è‰²
    
    # 1. æ‰€æœ‰ç»´åº¦ + Overall çš„ç»¼åˆç®±çº¿å›¾ (2x3 = 6ä¸ªå­å›¾)
    all_dims = DIMENSIONS + ["overall"]  # 5ä¸ªç»´åº¦ + 1ä¸ªoverall
    fig, axes = plt.subplots(2, 3, figsize=(16, 11))
    fig.suptitle('Agent Performance Across All Dimensions', fontsize=16, fontweight='bold')
    axes = axes.flatten()
    
    for idx, dim in enumerate(all_dims):
        # ç¡®å®šåˆ—å
        col = "overall_score" if dim == "overall" else f"{dim}_score"
        
        if col in df.columns:
            scores = df[col].dropna()
            if len(scores) > 0:
                if SEABORN_AVAILABLE:
                    sns.boxplot(y=scores, ax=axes[idx], color=color)
                    sns.swarmplot(y=scores, ax=axes[idx], color='black', alpha=0.2, size=2)
                else:
                    bp = axes[idx].boxplot([scores], patch_artist=True)
                    bp['boxes'][0].set_facecolor(color)
                    bp['boxes'][0].set_alpha(0.7)
                
                title = DIMENSION_NAMES.get(dim, dim)
                # Overallç”¨ä¸åŒé¢œè‰²èƒŒæ™¯çªå‡ºæ˜¾ç¤º
                if dim == "overall":
                    axes[idx].set_facecolor('#f0f0f0')
                    title = "â˜… " + title
                axes[idx].set_title(title, fontweight='bold')
                axes[idx].set_ylabel('Score (1-10)')
                axes[idx].set_ylim(0, 11)
                axes[idx].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    output_file = output_dir / "boxplot_all_dimensions.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜: {output_file}")
    plt.close()
    
    # 2. OverallåŠ æƒå¹³å‡è¯„åˆ†å¯¹æ¯”å›¾ï¼ˆå•ç‹¬å¤§å›¾ï¼‰
    fig, ax = plt.subplots(figsize=(8, 7))
    
    col = "overall_score"
    if col in df.columns:
        scores = df[col].dropna()
        if len(scores) > 0:
            if SEABORN_AVAILABLE:
                sns.boxplot(y=scores, ax=ax, color=color, width=0.4)
                sns.swarmplot(y=scores, ax=ax, color='black', alpha=0.2, size=3)
            else:
                bp = ax.boxplot([scores], patch_artist=True)
                bp['boxes'][0].set_facecolor(color)
                bp['boxes'][0].set_alpha(0.7)
            
            ax.set_title('Agent Overall Score', fontsize=14, fontweight='bold')
            ax.set_ylabel('Score (1-10)', fontsize=12)
            ax.set_ylim(0, 11)
            ax.grid(True, alpha=0.3, axis='y')
            
            stats_text = f"Î¼={scores.mean():.2f}, Ïƒ={scores.std():.2f}, n={len(scores)}"
            ax.text(0.02, 0.98, stats_text, transform=ax.transAxes,
                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                   fontsize=10)
    
    plt.tight_layout()
    output_file = output_dir / "boxplot_overall.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜: {output_file}")
    plt.close()
    
    # 3. é›·è¾¾å›¾ï¼ˆå„ç»´åº¦å¹³å‡åˆ†ï¼‰
    create_radar_chart(df, output_dir)


def create_radar_chart(df: pd.DataFrame, output_dir: Path):
    """ç”Ÿæˆé›·è¾¾å›¾"""
    if not PLOT_AVAILABLE:
        return
    
    print("\nğŸ“Š ç”Ÿæˆé›·è¾¾å›¾")
    
    # è®¡ç®—å„ç»´åº¦çš„å¹³å‡åˆ†
    means = []
    for dim in DIMENSIONS:
        col = f"{dim}_score"
        if col in df.columns:
            score = df[col].dropna().mean()
            means.append(score if not np.isnan(score) else 0)
        else:
            means.append(0)
    
    # é›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(DIMENSIONS), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))
    
    color = '#3498db'
    
    values = means + means[:1]  # é—­åˆ
    ax.plot(angles, values, 'o-', linewidth=2, label='Agent', color=color)
    ax.fill(angles, values, alpha=0.25, color=color)
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels([DIMENSION_NAMES[d] for d in DIMENSIONS], fontsize=10)
    ax.set_ylim(0, 10)
    ax.set_title('Agent Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
    ax.grid(True)
    
    plt.tight_layout()
    output_file = output_dir / "radar_chart.png"
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"âœ… ä¿å­˜: {output_file}")
    plt.close()


def run_analysis(df: pd.DataFrame, output_dir: Path):
    """è¿è¡Œåˆ†ææµç¨‹"""
    print("\n" + "="*80)
    print("ğŸ“Š å¼€å§‹åˆ†æè¯„ä¼°ç»“æœ")
    print("="*80)
    
    # æ–‡æœ¬ç»Ÿè®¡åˆ†æ
    analyze_scores(df)
    score_distribution(df)
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if PLOT_AVAILABLE:
        create_boxplots(df, output_dir)
        print(f"\nğŸ“ æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
async def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description="Agentè¯„ä¼°è„šæœ¬ - åˆå¹¶ã€è¯„ä¼°ã€åˆ†æAgentå†³ç­–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python evaluate.py
  python evaluate.py --agent-csv path/to/agent_results.csv
  python evaluate.py --agent-csv agent.csv --list-final ground_truth.xlsx --output-dir my_eval
        """
    )
    
    parser.add_argument(
        '--agent-csv',
        type=str,
        default='agent_results.csv',
        help='Agentç»“æœCSVæ–‡ä»¶è·¯å¾„ (é»˜è®¤: agent_results.csv)'
    )
    
    parser.add_argument(
        '--list-final',
        type=str,
        default='agent_eval/ground_truth/ListFinalnew.xlsx',
        help='Ground Truth Excelæ–‡ä»¶è·¯å¾„ (é»˜è®¤: agent_eval/ground_truth/ListFinalnew.xlsx)'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default='agent_eval/results',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: agent_eval/results)'
    )
    
    parser.add_argument(
        '--ground-truth-column',
        type=str,
        default='ground_truth_eng',
        help='Ground truthåˆ—å (é»˜è®¤: ground_truth_eng)'
    )
    
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸ¥ Agentä¸´åºŠå†³ç­–è¯„ä¼°å·¥å…·")
    print("="*80)
    print(f"ğŸ“„ Agent CSV: {args.agent_csv}")
    print(f"ğŸ“„ Ground Truth: {args.list_final}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ“Š è¯„ä¼°ç»´åº¦: {', '.join(DIMENSION_NAMES.values())}")
    print("="*80)
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ­¥éª¤1: åˆå¹¶æ•°æ®
    merged_df = merge_agent_and_ground_truth(args.agent_csv, args.list_final)
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    merged_output = output_dir / "merged_agent_results.csv"
    merged_df.to_csv(merged_output, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ åˆå¹¶ç»“æœå·²ä¿å­˜: {merged_output}")
    
    # æ­¥éª¤2: æ£€æŸ¥æ˜¯å¦å·²æœ‰è¯„ä¼°ç»“æœ
    eval_output = output_dir / "evaluation_results.csv"
    
    if eval_output.exists():
        print(f"\nå‘ç°å·²æœ‰è¯„ä¼°ç»“æœ: {eval_output}")
        user_input = input("æ˜¯å¦é‡æ–°è¯„ä¼°? (y/n, é»˜è®¤nè·³è¿‡è¯„ä¼°ç›´æ¥åˆ†æ): ").strip().lower()
        
        if user_input == 'y':
            eval_df = await run_evaluation(merged_df, args.ground_truth_column)
            if not eval_df.empty:
                eval_df.to_csv(eval_output, index=False, encoding="utf-8-sig")
                print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_output}")
        else:
            print("è·³è¿‡è¯„ä¼°ï¼Œç›´æ¥åŠ è½½å·²æœ‰ç»“æœè¿›è¡Œåˆ†æ...")
            eval_df = pd.read_csv(eval_output, encoding="utf-8")
    else:
        # è¿è¡Œè¯„ä¼°
        eval_df = await run_evaluation(merged_df, args.ground_truth_column)
        if not eval_df.empty:
            eval_df.to_csv(eval_output, index=False, encoding="utf-8-sig")
            print(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_output}")
    
    if eval_df.empty:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¾›åˆ†æ")
        return
    
    # æ­¥éª¤3: è¿è¡Œåˆ†æ
    # figuresç›®å½•åœ¨agent_evalä¸‹ï¼Œä¸resultså¹³çº§
    figures_dir = output_dir.parent / "figures" if output_dir.name == "results" else output_dir / "figures"
    run_analysis(eval_df, figures_dir)
    
    print("\n" + "="*80)
    print("âœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    print("="*80)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   - merged_agent_results.csv (åˆå¹¶æ•°æ®)")
    print(f"   - evaluation_results.csv (è¯„ä¼°ç»“æœ)")
    print(f"   - figures/ (å¯è§†åŒ–å›¾è¡¨)")


if __name__ == "__main__":
    asyncio.run(main())
